
# Why is deep-learning so rich of Expressivty?

### On Representation Power of Neural Network
> Before start, I would like to borrow a researcher's comment- How deep-learning work so well still in its infancy.. Yes, we still lack of theoretical understanding on defining what is neural nets' expressivity. The defintion is necessary and valuable since the clear definition of it will provides useful guidelines to our decision about which neural network we shall use.
----

### What is this repository for?
> This repo intends to contrive an **efficient and convenient answer** to the question: "Which form of neural architecture is required to solve the given task/problem?"  

### Relevant Literatures

> 1. Neural networks and rational functions (2017, M.Telgarsky) https://arxiv.org/pdf/1706.03301.pdf <br/>
>> Major thesis :  "As a final piece of the story, note that the conversion between rational functions and ReLU networks is more seamless if instead one converts to rational networks, meaning neural networks where each activation function is a rational
function."


### Must Read Article 
> 1. What is Function Class?
https://en.wikipedia.org/wiki/Learnable_function_class


### Refer to
1. https://arxiv.org/pdf/1708.02691.pdf


$\Bbb N$

# This topic is relevant to a famous phenomenal term - curse of dimensionality

T. Poggio, H. Mhaskar, L. Rosasco, B. Miranda, and Q. Liao, Why and when can deep-but
not shallow-networks avoid the curse of dimensionality: a review, International Journal of
Automation and Computing, 14 (2017), pp. 503–519.

P. Grohs, F. Hornung, A. Jentzen, and P. Von Wurstemberger, A proof that artificial
neural networks overcome the curse of dimensionality in the numerical approximation of
black-scholes partial differential equations, arXiv preprint arXiv:1809.02362, (2018).


# On the apsect of apprximatied target function's frequency 
Frequency principle: Fourier analysis sheds light on deep neural networks

# Special Case of MLP
A. Pinkus, Approximation theory of the MLP model in neural networks, Acta Numerica, 8
(1999), pp. 143–195.

# On Art of Computability/Trainability 
A. Blum and R. L. Rivest, Training a 3-node neural network is np-complete, in Advances in
Neural Information Processing Systems, 1989, pp. 494–501.
